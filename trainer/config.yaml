model: gpt2
user_name: Kasdeja23
model_name: GPT2WaP
dataconfig:
  data_url: https://www.gutenberg.org/cache/epub/2600/pg2600.txt
  split: 0.9
  min_context_len: 64
  max_context_len: 64
trainer_args:
  output_dir: checkpoints
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  evaluation_strategy: steps
  eval_accumulation_steps: 4
  eval_steps: 10
  logging_steps: 1
  gradient_accumulation_steps: 4
  num_train_epochs: 20
  weight_decay: 0.01
  learning_rate: 0.0001
  save_steps: 30
  fp16: True
  seed: 42
  save_total_limit: 1
  warmup_steps: 0
  # load_best_model_at_end: True
  # max_grad_norm: 2
  # lr_scheduler_type: cosine